# This complete pipeline perform: 
# - filling missing data with FillImputer
# - train the machine with PCA() and Logistic Classifier, with L2 regularization as penalty
# (lambda is tuned by optimizing AUC)
# - save prediction in a CSV file

# Import usefull packages
begin
    using Markdown, InteractiveUtils, Pkg
    Pkg.activate(joinpath(Pkg.devdir(), "MLCourse"))
    import Pkg;Pkg.add("XGBoost")
    using DataFrames, CSV, MLJ, MLJLinearModels, OpenML, MLCourse, MLJMultivariateStatsInterface, LinearAlgebra, Random, StatsBase, MLJDecisionTreeInterface, XGBoost
    #using RDatasets, RandomForests
end

# Import training & test data and deal with missing data with FillImputer
begin
	weather = CSV.read(joinpath(@__DIR__, "..",  "data", "trainingdata.csv"), DataFrame)
    coerce!(weather, :precipitation_nextday=>OrderedFactor{2})
    weather_filled = MLJ.transform(fit!(machine(FillImputer(), weather)), weather) #Filling imputer data during the fit 
end

# Standardization
# n'est pas utilisé après car contient des Infs or NaNs
# verbosity pour afficher les moyennes et SD pour voir ou est l'erreur qui produit le NaN 
# on ignore ALT_sunshine_4 car cree un NaN en divisant avec SD=0
begin
    stan_mach = machine(Standardizer(features = [:ALT_sunshine_4], ignore = true), select(weather_filled, Not(:precipitation_nextday))) 
    fit!(stan_mach, verbosity=2) 
    MLJ.transform(stan_mach, weather_filled)
    input = select(weather_filled, Not(:precipitation_nextday))
    output = weather_filled.precipitation_nextday
end

#coerce the multi class so that they have an ordering
#coerce!(output,binary)
#output = coerce(weather_filled.precipitation_nextday, OrderedFactor)

# separate into training and validation sets
function partitionTrainValidation(data, at = 0.5) 
    n = nrow(data)
    idx = shuffle(1:n)
    train_idx = view(idx, 1:floor(Int, at*n))
    val_idx = view(idx, (floor(Int, at*n)+1):n)
    data[train_idx,:], data[val_idx,:]
end

begin
	train,validation = partitionTrainValidation(weather_filled,0.5)
	train_input = select(train, Not(:precipitation_nextday)) 
    train_output = train.precipitation_nextday
	val_input = select(validation, Not(:precipitation_nextday))
    val_output = validation.precipitation_nextday 
end

"""#fitting on training, evaluation on validation set
begin
    rfr_model = RandomForestRegressor(n_trees = 50)
    #rfr_model = RandomForestClassifier(n_trees = 50)
    rfr_mach = fit!(machine(rfr_model,train_input,train_output))
    pred = predict(rfr_mach,val_input) #predict ne marche pas... 
end"""

begin
    test_data = CSV.read(joinpath(@__DIR__, "..",  "data", "testdata.csv"), DataFrame)
	cleaned_test_data = dropmissing(test_data)
end

#random forest classification/regression
random_forest_mach = fit!(machine(RandomForestClassifier(n_trees = 600), input, output))
evaluate!(random_forest_mach, resampling=CV(), measure = auc)
#auc_ = MLJ.auc(predict(selftuning_lambda,input),output)

"""#ca serait plus correcte de faire de cette maniere mais je n'arrive pas a faire une evaluation
random_forest_reg_mach = fit!(machine(RandomForestRegressor(n_trees = 600), input, output))
evaluate!(random_forest_reg_mach, resampling=CV(nfolds = 10), measure = rmse)"""

#generation of test output file
#ATTENTION: avec ou sans standardizer() ca retourne exactement la meme valeur pour tous les inputs (decalage)
begin
	probs = MLJ.predict(random_forest_mach, cleaned_test_data)
	N = size(cleaned_test_data)[1]
	df_L2 = DataFrame(id = 1:N, precipitation_nextday = probs[2])
    CSV.write(joinpath(@__DIR__, "..", "results", "random_forest_500_take5.csv"), df_L2)
	# CSV.write(joinpath(@__DIR__, "..", "results", "random_forest_with_standardization.csv"), df_L2)
	# confusion_matrix(predict_mode(random_forest_mach, input), output)
end